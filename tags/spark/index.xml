<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on Coding notes - cheat sheets and snippets</title>
    <link>https://www.bustroker.com/tags/spark/</link>
    <description>Recent content in Spark on Coding notes - cheat sheets and snippets</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Dec 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.bustroker.com/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spark with Scala and Azure Storage</title>
      <link>https://www.bustroker.com/notes/spark-with-scala-and-azure-storage/</link>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://www.bustroker.com/notes/spark-with-scala-and-azure-storage/</guid>
      <description>Goal Set up a Spark project in Scala that reads from Azure Blob Storage and writes to Azure Data Lake Storage Gen2.&#xA;Dependencies ThisBuild / version := &amp;#34;0.1.0&amp;#34; ThisBuild / scalaVersion := &amp;#34;2.12.13&amp;#34; ThisBuild / organization := &amp;#34;bkr&amp;#34; lazy val KafkaStreamProcessing = (project in file(&amp;#34;.&amp;#34;)) .settings( name := &amp;#34;Bkr.Spark&amp;#34;, libraryDependencies += &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-core&amp;#34; % &amp;#34;3.1.1&amp;#34;, libraryDependencies += &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-sql&amp;#34; % &amp;#34;3.1.1&amp;#34;, libraryDependencies += &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-sql-kafka-0-10&amp;#34; % &amp;#34;3.1.1&amp;#34;, libraryDependencies += &amp;#34;org.</description>
    </item>
    <item>
      <title>Writing to Delta Lake from Spark with Scala</title>
      <link>https://www.bustroker.com/notes/writing-to-delta-lake-from-spark-with-scala/</link>
      <pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://www.bustroker.com/notes/writing-to-delta-lake-from-spark-with-scala/</guid>
      <description>Goal Process JSON operations (create/update) and write the final snapshot to Delta Lake on Azure Data Lake Storage.&#xA;Dependencies ThisBuild / version := &amp;#34;0.1.0&amp;#34; ThisBuild / scalaVersion := &amp;#34;2.13.8&amp;#34; ThisBuild / organization := &amp;#34;bkr&amp;#34; libraryDependencies ++= List(&amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-core&amp;#34; % &amp;#34;3.2.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-sql&amp;#34; % &amp;#34;3.2.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-sql-kafka-0-10&amp;#34; % &amp;#34;3.2.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-avro&amp;#34; % &amp;#34;3.2.0&amp;#34;) libraryDependencies ++= List(&amp;#34;org.apache.hadoop&amp;#34; % &amp;#34;hadoop-common&amp;#34; % &amp;#34;3.3.1&amp;#34;, &amp;#34;org.apache.hadoop&amp;#34; % &amp;#34;hadoop-azure&amp;#34; % &amp;#34;3.3.1&amp;#34;) libraryDependencies += &amp;#34;org.</description>
    </item>
    <item>
      <title>Run Jupyter Notebook with Spark in Docker</title>
      <link>https://www.bustroker.com/notes/run-jupyter-notebook-with-spark-in-docker/</link>
      <pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://www.bustroker.com/notes/run-jupyter-notebook-with-spark-in-docker/</guid>
      <description>Goal Run a Jupyter notebook with Apache Spark support using Docker, enabling Scala code execution and Spark monitoring.&#xA;Setup Pull and run the Spark-enabled Jupyter container:&#xA;docker run -p 8888:8888 -p 4041:4041 jupyter/all-spark-notebook This exposes:&#xA;Port 8888: Jupyter interface Port 4041: Spark WebUI Access Jupyter Copy the authentication token from the terminal output Navigate to localhost:8888 in your browser Authenticate using the token Write Spark Code Create a new notebook with the spylon-kernel kernel to write Scala code with Spark.</description>
    </item>
    <item>
      <title>Install Spark and Hadoop on Windows 10</title>
      <link>https://www.bustroker.com/notes/install-spark-and-hadoop-on-windows-10/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://www.bustroker.com/notes/install-spark-and-hadoop-on-windows-10/</guid>
      <description>Goal Run Apache Spark locally on Windows 10 with Hadoop binaries.&#xA;Prerequisites Verify Java and Scala installations:&#xA;java -version openjdk version &amp;#34;1.8.0_292&amp;#34; OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_292-b10) OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.292-b10, mixed mode) sbt console Welcome to Scala 2.12.13 (OpenJDK 64-Bit Server VM, Java 1.8.0_292). Install Spark Download from https://spark.apache.org/downloads.html:&#xA;Spark release: 3.2.0 Package type: Pre-built for Apache Hadoop 2.7 Extract to C:\Spark\spark-3.2.0-bin-hadoop2.7&#xA;Set environment variables:&#xA;setx SPARK_HOME &amp;#34;C:\Spark\spark-3.</description>
    </item>
  </channel>
</rss>
